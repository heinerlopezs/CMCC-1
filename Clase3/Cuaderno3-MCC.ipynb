{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9f5b4e",
   "metadata": {},
   "source": [
    "### **Transformers y atención como bloque universal** \n",
    "\n",
    "Este cuaderno cubre, con implementaciones **desde cero en PyTorch** :\n",
    "\n",
    "- **Self-attention (autoatención)**: consultas (**Q**), claves (**K**) y valores (**V**), incluyendo su **complejidad cuadrática**.\n",
    "- **Multi-head attention**, **LayerNorm** y **conexiones residuales**.\n",
    "- **Arquitectura encoder/decoder** y variantes (**encoder-only**, **decoder-only para LLM**).\n",
    "- **Codificación posicional** y variantes modernas (**sin/cos**, **learned**, **RoPE**, **ALiBi**).\n",
    "\n",
    "> Enfoque: cuaderno didáctico, ejecutable en CPU, usando un corpus pequeño y un tokenizador simple en Python (sin `torchtext`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a2465",
   "metadata": {},
   "source": [
    "#### **Estructura del cuaderno**\n",
    "\n",
    "1. Preparación del entorno y tokenización simple (sin `torchtext`).\n",
    "2. Codificaciones posicionales: sinusoidal, learned, **RoPE** y **ALiBi**.\n",
    "3. Self-attention (Q, K, V), máscara causal y complejidad $O(T^2)$.\n",
    "4. Multi-head attention + residual + LayerNorm + FFN (bloque Transformer).\n",
    "5. Encoder vs. Decoder y variantes (encoder-only / decoder-only).\n",
    "6. Mini-entrenamiento autoregresivo (decoder-only) con `AdamW`, warmup y clipping.\n",
    "7. Decodificación básica (temperatura, top-k, top-p).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d9e87",
   "metadata": {},
   "source": [
    "##### **Cómo trabajar este cuaderno en clase**\n",
    "\n",
    "- Las celdas de implementación están listas para ejecutar.\n",
    "- Las secciones **Actividad guiada** son para discusión y respuesta escrita.\n",
    "- Las celdas **Respuesta del estudiante** se pueden editar directamente en clase o como tarea.\n",
    "- Las celdas **Extensión opcional** permiten experimentar sin modificar el código base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63099569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ade108",
   "metadata": {},
   "source": [
    "#### **1. Tokenización simple y vocabulario**\n",
    "\n",
    "Para mantener el cuaderno autocontenido, usaremos un tokenizador básico por espacios y un vocabulario propio.\n",
    "\n",
    "Esto es suficiente para ilustrar el pipeline de un Transformer:\n",
    "\n",
    "**texto -> tokens -> ids -> embeddings -> codificación posicional -> bloques Transformer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"the llama learns quickly\",\n",
    "    \"the llama runs fast\",\n",
    "    \"the dog runs fast\",\n",
    "    \"the dog barks loudly\",\n",
    "    \"the horse runs fast\",\n",
    "    \"the horse eats hay\",\n",
    "    \"the llama eats hay\",\n",
    "    \"attention is a universal block\",\n",
    "    \"transformers use self attention\",\n",
    "    \"decoder only models predict next token\",\n",
    "    \"encoder decoder models use cross attention\",\n",
    "]\n",
    "\n",
    "def basic_tokenize(text: str) -> List[str]:\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "class SimpleVocab:\n",
    "    def __init__(self, token_lists: List[List[str]], min_freq: int = 1):\n",
    "        specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "        counter = Counter(tok for toks in token_lists for tok in toks)\n",
    "        self.itos = specials[:]\n",
    "        for tok, freq in counter.items():\n",
    "            if freq >= min_freq and tok not in specials:\n",
    "                self.itos.append(tok)\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "        self.pad_id = self.stoi[\"<pad>\"]\n",
    "        self.bos_id = self.stoi[\"<bos>\"]\n",
    "        self.eos_id = self.stoi[\"<eos>\"]\n",
    "        self.unk_id = self.stoi[\"<unk>\"]\n",
    "\n",
    "    def encode(self, text: str, add_bos: bool = True, add_eos: bool = True) -> List[int]:\n",
    "        ids = []\n",
    "        if add_bos:\n",
    "            ids.append(self.bos_id)\n",
    "        ids.extend(self.stoi.get(tok, self.unk_id) for tok in basic_tokenize(text))\n",
    "        if add_eos:\n",
    "            ids.append(self.eos_id)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        toks = []\n",
    "        for idx in ids:\n",
    "            tok = self.itos[idx]\n",
    "            if tok in {\"<pad>\", \"<bos>\", \"<eos>\"}:\n",
    "                continue\n",
    "            toks.append(tok)\n",
    "        return \" \".join(toks)\n",
    "\n",
    "token_lists = [basic_tokenize(x) for x in corpus]\n",
    "vocab = SimpleVocab(token_lists)\n",
    "vocab_size = len(vocab.itos)\n",
    "\n",
    "print(\"Tamaño del vocabulario:\", vocab_size)\n",
    "print(\"Muestra del vocabulario:\", vocab.itos[:20])\n",
    "\n",
    "sample_text = \"the llama runs fast\"\n",
    "sample_ids = vocab.encode(sample_text)\n",
    "print(\"Texto:\", sample_text)\n",
    "print(\"Codificado:\", sample_ids)\n",
    "print(\"Decodificado:\", vocab.decode(sample_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206743f0",
   "metadata": {},
   "source": [
    "#### **Actividad guiada 1 - Pipeline de entrada (texto, no código)**\n",
    "\n",
    "Responde de forma argumentada:\n",
    "\n",
    "1. Explica el rol de cada etapa del pipeline:\n",
    "   **texto -> tokens -> ids -> embeddings -> codificación posicional -> bloque Transformer**.  \n",
    "   Indica qué información se conserva y qué información se transforma en cada paso.\n",
    "\n",
    "2. ¿Por qué usar tokens especiales como `<bos>`, `<eos>`, `<pad>` y `<unk>`?  \n",
    "   Describe un caso concreto donde cada uno sea necesario.\n",
    "\n",
    "3. Si el vocabulario se construye con un corpus pequeño, ¿qué limitaciones aparecen al generar texto o evaluar ejemplos nuevos?\n",
    "\n",
    "4. Compara tokenización por espacios vs. subword tokenization (BPE/WordPiece/SentencePiece).  \n",
    "   ¿Qué problema resuelve la tokenización por subpalabras?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678b9f2",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 1**\n",
    "\n",
    "Escribe aquí tus respuestas (texto):\n",
    "- **1. Pipeline:**  \n",
    "- **2. Tokens especiales:**  \n",
    "- **3. Limitaciones del vocabulario pequeño:**  \n",
    "- **4. Comparación con subwords:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensión opcional (estudiante)\n",
    "# Prueba agregar nuevas oraciones al corpus y vuelve a construir el vocabulario.\n",
    "# Luego compara:\n",
    "# - tamaño del vocabulario\n",
    "# - presencia de tokens <unk>\n",
    "#\n",
    "# Sugerencia:\n",
    "# corpus_ext = corpus + [\"...\",\"...\"]\n",
    "# token_lists_ext = [basic_tokenize(x) for x in corpus_ext]\n",
    "# vocab_ext = SimpleVocab(token_lists_ext)\n",
    "# print(\"Nuevo tamaño:\", len(vocab_ext.itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627f228",
   "metadata": {},
   "source": [
    "#### **2. Codificación posicional y variantes**\n",
    "\n",
    "##### **2.1 Sinusoidal (absoluto)**\n",
    "Es la codificación posicional clásica del Transformer original. No introduce parámetros extra y permite codificar posición con funciones seno/coseno.\n",
    "\n",
    "##### **2.2 Learned positional embeddings (absoluto)**\n",
    "Se aprende un vector por posición. Suele funcionar bien, aunque extrapola peor fuera del rango visto durante entrenamiento.\n",
    "\n",
    "##### **2.3 RoPE (Rotary Positional Embedding)**\n",
    "Rota pares de dimensiones de **Q** y **K** según la posición, inyectando posición de manera **relativa** en la atención. Es muy usado en LLMs modernos.\n",
    "\n",
    "##### **2.4 ALiBi**\n",
    "Añade un **sesgo lineal** a los logits de atención según la distancia entre posiciones. Favorece relaciones locales y suele escalar bien a contextos largos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # [1, T, D]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, T, D]\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, D = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        return x + self.pos_emb(pos)\n",
    "\n",
    "# Comparación visual\n",
    "d_model = 32\n",
    "T_vis = 64\n",
    "dummy = torch.zeros(1, T_vis, d_model)\n",
    "\n",
    "sin_pe = SinusoidalPositionalEncoding(d_model, max_len=256)\n",
    "learned_pe = LearnedPositionalEncoding(d_model, max_len=256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sin_out = sin_pe(dummy)[0].cpu()\n",
    "    learned_out = learned_pe(dummy)[0].cpu()\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(sin_out.T, aspect=\"auto\")\n",
    "plt.title(\"Codificación posicional sinusoidal (T x D)\")\n",
    "plt.xlabel(\"Posición\")\n",
    "plt.ylabel(\"Dimensión\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(learned_out.T, aspect=\"auto\")\n",
    "plt.title(\"Embeddings posicionales aprendidos (T x D)\")\n",
    "plt.xlabel(\"Posición\")\n",
    "plt.ylabel(\"Dimensión\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146d496",
   "metadata": {},
   "source": [
    "\n",
    "##### **2.5 Implementación de RoPE y ALiBi (bloques reutilizables)**\n",
    "\n",
    "A continuación implementamos funciones reutilizables para:\n",
    "\n",
    "- aplicar **RoPE** sobre tensores `[..., T, D]` (con `D` par),\n",
    "- construir el sesgo de atención de **ALiBi** por cabecera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eed915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    # x: [..., D], D debe ser par\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    out = torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
    "    return out\n",
    "\n",
    "def build_rope_cache(seq_len: int, dim: int, device=None, base: float = 10000.0):\n",
    "    assert dim % 2 == 0, \"RoPE requiere dimensión par por cabecera\"\n",
    "    pos = torch.arange(seq_len, dtype=torch.float32, device=device).unsqueeze(1)      # [T,1]\n",
    "    freq = torch.arange(0, dim, 2, dtype=torch.float32, device=device)                # [D/2]\n",
    "    inv_freq = 1.0 / (base ** (freq / dim))                                            # [D/2]\n",
    "    angles = pos * inv_freq.unsqueeze(0)                                               # [T,D/2]\n",
    "    # Expandir a dimensión completa intercalando\n",
    "    cos = torch.repeat_interleave(torch.cos(angles), repeats=2, dim=-1)                # [T,D]\n",
    "    sin = torch.repeat_interleave(torch.sin(angles), repeats=2, dim=-1)                # [T,D]\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    # x: [B, H, T, D]\n",
    "    # cos/sin: [T, D]\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # [1,1,T,D]\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    return (x * cos) + (_rotate_half(x) * sin)\n",
    "\n",
    "def build_alibi_bias(num_heads: int, seq_len: int, device=None) -> torch.Tensor:\n",
    "    # Pendientes geométricas simples (versión didáctica)\n",
    "    slopes = torch.tensor([2 ** (-(i + 1) / num_heads) for i in range(num_heads)],\n",
    "                          dtype=torch.float32, device=device)  # [H]\n",
    "    i = torch.arange(seq_len, device=device)\n",
    "    j = torch.arange(seq_len, device=device)\n",
    "    dist = (i[None, :] - j[:, None]).abs().float()   # [T,T]\n",
    "    # Forma del sesgo [1, H, T, T]\n",
    "    bias = -slopes.view(1, num_heads, 1, 1) * dist.view(1, 1, seq_len, seq_len)\n",
    "    return bias\n",
    "\n",
    "# Tensores de demostración\n",
    "B, H, T, Dh = 1, 2, 8, 8\n",
    "q = torch.randn(B, H, T, Dh)\n",
    "k = torch.randn(B, H, T, Dh)\n",
    "\n",
    "cos, sin = build_rope_cache(seq_len=T, dim=Dh, device=q.device)\n",
    "q_rope = apply_rope(q, cos, sin)\n",
    "k_rope = apply_rope(k, cos, sin)\n",
    "\n",
    "alibi = build_alibi_bias(num_heads=H, seq_len=T, device=q.device)\n",
    "\n",
    "print(\"Forma de q:\", q.shape)\n",
    "print(\"Forma de q_rope:\", q_rope.shape)\n",
    "print(\"Forma del sesgo ALiBi:\", alibi.shape)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(alibi[0, 0].cpu(), aspect=\"auto\")\n",
    "plt.title(\"Sesgo ALiBi (cabecera 0)\")\n",
    "plt.xlabel(\"Posición clave\")\n",
    "plt.ylabel(\"Posición consulta\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d3eec",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 2 - Posición en Transformers (texto, no código)**\n",
    "\n",
    "1. Explica con tus palabras por qué un mecanismo de atención puro **no conoce el orden** de los tokens.\n",
    "\n",
    "2. Compara conceptualmente:\n",
    "   - **Sinusoidal** (absoluto)\n",
    "   - **Learned positional embeddings** (absoluto)\n",
    "   - **RoPE** (posición incorporada en Q/K)\n",
    "   - **ALiBi** (sesgo en logits)\n",
    "\n",
    "   Para cada uno, menciona:\n",
    "   - dónde se aplica,\n",
    "   - si agrega parámetros,\n",
    "   - una ventaja y una posible limitación.\n",
    "\n",
    "3. ¿Por qué RoPE y ALiBi suelen aparecer en modelos modernos orientados a contextos largos?\n",
    "\n",
    "4. Si un modelo fue entrenado con longitud máxima 512, ¿qué problemas podrían aparecer al usar secuencias mucho más largas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e73a9",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante- Actividad 2**\n",
    "\n",
    "Completa esta comparación (texto libre o tabla):\n",
    "\n",
    "- **Sinusoidal:**  \n",
    "- **Learned:**  \n",
    "- **RoPE:**  \n",
    "- **ALiBi:**  \n",
    "\n",
    "**Conclusión personal:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd048a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensión opcional (estudiante)\n",
    "# Grafica el sesgo ALiBi para distintas cabeceras y comenta cómo cambia.\n",
    "# Sugerencia:\n",
    "# bias = build_alibi_bias(num_heads=4, seq_len=16)\n",
    "# plt.imshow(bias[0, 0].cpu()); plt.colorbar(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e955410",
   "metadata": {},
   "source": [
    "#### **3. Self-attention: Q, K, V y complejidad $O(T^2)$**\n",
    "\n",
    "Recordatorio (por cabecera):\n",
    "$$\n",
    "\\text{Atencion}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + \\text{mask}\\right)V\n",
    "$$\n",
    "\n",
    "- **Q (queries)**: qué busca cada token.\n",
    "- **K (keys)**: cómo se indexa cada token.\n",
    "- **V (values)**: contenido que se mezcla.\n",
    "- La matriz de atención tiene tamaño **T×T**, por eso el costo escala como **$O(T^2)$** en secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db823b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_causal_mask(T: int, device=None) -> torch.Tensor:\n",
    "    # True significa \"enmascarar\"\n",
    "    return torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, attn_mask=None, extra_bias=None, dropout_p=0.0, training=False):\n",
    "    '''\n",
    "    q, k, v: [B, H, T, D]\n",
    "    attn_mask: [T, T] bool (True = enmascarado)\n",
    "    extra_bias: [1, H, T, T] or broadcastable (e.g., ALiBi)\n",
    "    '''\n",
    "    d = q.size(-1)\n",
    "    scores = (q @ k.transpose(-2, -1)) / math.sqrt(d)   # [B,H,T,T]\n",
    "    if extra_bias is not None:\n",
    "        scores = scores + extra_bias\n",
    "    if attn_mask is not None:\n",
    "        scores = scores.masked_fill(attn_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    if dropout_p > 0:\n",
    "        attn = F.dropout(attn, p=dropout_p, training=training)\n",
    "    out = attn @ v                                        # [B,H,T,D]\n",
    "    return out, attn, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb45d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración: una secuencia con embeddings aleatorios proyectados a Q/K/V\n",
    "B, T, D_model = 1, 6, 32\n",
    "H = 4\n",
    "Dh = D_model // H\n",
    "\n",
    "x = torch.randn(B, T, D_model)\n",
    "\n",
    "Wq = nn.Linear(D_model, D_model, bias=False)\n",
    "Wk = nn.Linear(D_model, D_model, bias=False)\n",
    "Wv = nn.Linear(D_model, D_model, bias=False)\n",
    "\n",
    "q = Wq(x).view(B, T, H, Dh).transpose(1, 2)  # [B,H,T,Dh]\n",
    "k = Wk(x).view(B, T, H, Dh).transpose(1, 2)\n",
    "v = Wv(x).view(B, T, H, Dh).transpose(1, 2)\n",
    "\n",
    "causal_mask = make_causal_mask(T, device=x.device)\n",
    "\n",
    "# Variante A: base\n",
    "out_vanilla, attn_vanilla, _ = scaled_dot_product_attention(q, k, v, attn_mask=causal_mask)\n",
    "\n",
    "# Variante B: con RoPE + ALiBi\n",
    "cos, sin = build_rope_cache(seq_len=T, dim=Dh, device=x.device)\n",
    "q_rope = apply_rope(q, cos, sin)\n",
    "k_rope = apply_rope(k, cos, sin)\n",
    "alibi_bias = build_alibi_bias(H, T, device=x.device)\n",
    "\n",
    "out_modern, attn_modern, _ = scaled_dot_product_attention(\n",
    "    q_rope, k_rope, v, attn_mask=causal_mask, extra_bias=alibi_bias\n",
    ")\n",
    "\n",
    "print(\"Forma de salida (base):\", out_vanilla.shape)\n",
    "print(\"Forma de atención:\", attn_vanilla.shape)  # [B,H,T,T]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.imshow(attn_vanilla[0, 0].detach().cpu(), aspect=\"auto\")\n",
    "plt.title(\"Pesos de self-attention (cabecera 0, causal)\")\n",
    "plt.xlabel(\"Posición clave\")\n",
    "plt.ylabel(\"Posición consulta\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuición de complejidad: la matriz de logits de atención es T x T\n",
    "def attention_score_matrix_size(T: int, H: int = 8):\n",
    "    return H * T * T\n",
    "\n",
    "Ts = [16, 32, 64, 128, 256, 512]\n",
    "sizes = [attention_score_matrix_size(T, H=8) for T in Ts]\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(Ts, sizes, marker=\"o\")\n",
    "plt.title(\"Escala cuadrática del tamaño de logits de atención (8 cabeceras)\")\n",
    "plt.xlabel(\"Longitud de secuencia T\")\n",
    "plt.ylabel(\"Elementos en logits de atención (H*T*T)\")\n",
    "plt.show()\n",
    "\n",
    "# Pequeño benchmark de tiempo (apto para CPU)\n",
    "def benchmark_attention(T=128, D_model=128, H=8, reps=20, use_rope=False, use_alibi=False):\n",
    "    B = 4\n",
    "    Dh = D_model // H\n",
    "    x = torch.randn(B, T, D_model)\n",
    "    Wq, Wk, Wv = nn.Linear(D_model, D_model, bias=False), nn.Linear(D_model, D_model, bias=False), nn.Linear(D_model, D_model, bias=False)\n",
    "    q = Wq(x).view(B, T, H, Dh).transpose(1, 2)\n",
    "    k = Wk(x).view(B, T, H, Dh).transpose(1, 2)\n",
    "    v = Wv(x).view(B, T, H, Dh).transpose(1, 2)\n",
    "    bias = None\n",
    "    if use_rope:\n",
    "        cos, sin = build_rope_cache(T, Dh, device=x.device)\n",
    "        q = apply_rope(q, cos, sin)\n",
    "        k = apply_rope(k, cos, sin)\n",
    "    if use_alibi:\n",
    "        bias = build_alibi_bias(H, T, device=x.device)\n",
    "    mask = make_causal_mask(T)\n",
    "    # calentamiento\n",
    "    for _ in range(3):\n",
    "        _ = scaled_dot_product_attention(q, k, v, attn_mask=mask, extra_bias=bias)\n",
    "    start = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "    end = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "    if start is not None:\n",
    "        torch.cuda.synchronize()\n",
    "        start.record()\n",
    "        for _ in range(reps):\n",
    "            _ = scaled_dot_product_attention(q, k, v, attn_mask=mask, extra_bias=bias)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        ms = start.elapsed_time(end) / reps\n",
    "    else:\n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(reps):\n",
    "            _ = scaled_dot_product_attention(q, k, v, attn_mask=mask, extra_bias=bias)\n",
    "        ms = (time.perf_counter() - t0) * 1000 / reps\n",
    "    return ms\n",
    "\n",
    "for T in [64, 128, 256]:\n",
    "    ms = benchmark_attention(T=T, D_model=128, H=8, reps=10)\n",
    "    print(f\"T={T:>3} -> ~{ms:.2f} ms / forward (solo atención scaled-dot)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7a9a3",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 3 - Self-attention y complejidad**\n",
    "\n",
    "1. Interpreta Q, K y V en términos de una búsqueda de información:\n",
    "   - ¿qué representa una **query**?\n",
    "   - ¿qué representa una **key**?\n",
    "   - ¿qué representa un **value**?\n",
    "\n",
    "2. Explica la función de la **máscara causal** en un modelo autoregresivo.\n",
    "   ¿Qué error conceptual habría si el decoder pudiera ver tokens futuros durante entrenamiento?\n",
    "\n",
    "3. La matriz de atención por cabecera tiene tamaño **T × T**.\n",
    "   Explica por qué esto implica crecimiento cuadrático en memoria/cómputo y qué efecto tiene al pasar de $T=128$ a $T=1024$.\n",
    "\n",
    "4. Observa el benchmark de tiempo:\n",
    "   ¿por qué el tiempo real no crece exactamente como una curva matemática ideal, aunque la complejidad teórica sí sea cuadrática?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560f864",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 3**\n",
    "\n",
    "- **Q/K/V como búsqueda de información:**  \n",
    "- **Máscara causal:**  \n",
    "- **Complejidad $O(T^2)$:**  \n",
    "- **Diferencia entre teoría y benchmark:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensión opcional (estudiante)\n",
    "# Compara mapas de atención con y sin máscara causal.\n",
    "# Puedes reutilizar q, k, v y llamar scaled_dot_product_attention(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51e2fb",
   "metadata": {},
   "source": [
    "#### **4. Multi-head attention + residual + LayerNorm + FFN (bloque Transformer)**\n",
    "\n",
    "Ahora encapsulamos la atención en un módulo reusable:\n",
    "\n",
    "- proyecciones **Q/K/V**\n",
    "- **multi-head**\n",
    "- opción de **RoPE** y/o **ALiBi**\n",
    "- **dropout**\n",
    "- **residual + LayerNorm**\n",
    "- **FeedForward (MLP)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95250e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1, use_rope: bool = False, use_alibi: bool = False):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.use_rope = use_rope\n",
    "        self.use_alibi = use_alibi\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        # x: [B,T,D] -> [B,H,T,Dh]\n",
    "        B, T, D = x.shape\n",
    "        return x.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        # x: [B,H,T,Dh] -> [B,T,D]\n",
    "        B, H, T, Dh = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, H * Dh)\n",
    "\n",
    "    def forward(self, x_q, x_kv=None, attn_mask=None):\n",
    "        '''\n",
    "        x_q:  [B,Tq,D]\n",
    "        x_kv: [B,Tk,D] (if None, self-attention)\n",
    "        '''\n",
    "        if x_kv is None:\n",
    "            x_kv = x_q\n",
    "\n",
    "        q = self._split_heads(self.q_proj(x_q))\n",
    "        k = self._split_heads(self.k_proj(x_kv))\n",
    "        v = self._split_heads(self.v_proj(x_kv))\n",
    "\n",
    "        B, H, Tq, Dh = q.shape\n",
    "        Tk = k.size(2)\n",
    "\n",
    "        extra_bias = None\n",
    "        # RoPE suele aplicarse cuando Tq == Tk (self-attention)\n",
    "        if self.use_rope and Tq == Tk:\n",
    "            cos, sin = build_rope_cache(seq_len=Tk, dim=Dh, device=q.device)\n",
    "            q = apply_rope(q, cos, sin)\n",
    "            k = apply_rope(k, cos, sin)\n",
    "\n",
    "        if self.use_alibi:\n",
    "            extra_bias = build_alibi_bias(H, Tk, device=q.device)[:, :, :Tq, :Tk]\n",
    "\n",
    "        out, attn, _ = scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=attn_mask,\n",
    "            extra_bias=extra_bias,\n",
    "            dropout_p=self.dropout.p,\n",
    "            training=self.training\n",
    "        )\n",
    "        out = self._merge_heads(out)\n",
    "        out = self.o_proj(out)\n",
    "        return out, attn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''Bloque Pre-Norm: LN -> MHA -> residual -> LN -> FFN -> residual'''\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, use_rope=False, use_alibi=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout=dropout, use_rope=use_rope, use_alibi=use_alibi)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        h, attn = self.attn(self.ln1(x), attn_mask=attn_mask)\n",
    "        x = x + self.dropout(h)    # residual\n",
    "        x = x + self.ffn(self.ln2(x))  # residual\n",
    "        return x, attn\n",
    "\n",
    "# Demostración forward\n",
    "B, T, D = 2, 10, 64\n",
    "x_demo = torch.randn(B, T, D)\n",
    "block = TransformerBlock(d_model=D, n_heads=4, d_ff=256, use_rope=True, use_alibi=True)\n",
    "y_demo, attn_demo = block(x_demo, attn_mask=make_causal_mask(T))\n",
    "print(\"Salida del bloque:\", y_demo.shape, \"Atención:\", attn_demo.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c8e07",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 4 - Bloque Transformer universal**\n",
    "\n",
    "1. Explica por qué el bloque Transformer se considera **un bloque universal** reutilizable en encoder-only, decoder-only y encoder/decoder.\n",
    "\n",
    "2. Describe la función de cada componente en el bloque:\n",
    "   - Multi-head attention\n",
    "   - Conexión residual\n",
    "   - LayerNorm\n",
    "   - FFN (MLP)\n",
    "\n",
    "3. ¿Qué problema de entrenamiento ayudan a mitigar:\n",
    "   - las residuales,\n",
    "   - la normalización,\n",
    "   - el dropout?\n",
    "\n",
    "4. ¿Qué significa que el bloque implementado sea **Pre-Norm**?\n",
    "   ¿En qué orden se aplican LN, atención y FFN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67144a0d",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 4**\n",
    "\n",
    "- **Bloque universal:**  \n",
    "- **Rol de cada componente:**  \n",
    "- **Estabilidad del entrenamiento:**  \n",
    "- **Pre-Norm:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a67022",
   "metadata": {},
   "source": [
    "#### **5. Encoder/Decoder y variantes (encoder-only, decoder-only)**\n",
    "\n",
    "##### **Bloque encoder**\n",
    "- Self-attention **no causal** (puede mirar toda la secuencia)\n",
    "- Útil para **representaciones contextualizadas** (BERT, clasificación, MLM, etc.)\n",
    "\n",
    "##### **Bloque decoder**\n",
    "- Self-attention **causal** (máscara triangular)\n",
    "- Opcionalmente **cross-attention (atención cruzada)** si hay un encoder (arquitectura encoder/decoder)\n",
    "- Base de modelos **decoder-only** para LLM (next-token prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840497d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block = TransformerBlock(d_model, n_heads, d_ff, dropout=dropout, use_rope=False, use_alibi=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention completo, sin máscara causal\n",
    "        return self.block(x, attn_mask=None)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, use_rope=True, use_alibi=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=dropout, use_rope=use_rope, use_alibi=use_alibi)\n",
    "\n",
    "        self.ln_cross = nn.LayerNorm(d_model)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout=dropout, use_rope=False, use_alibi=False)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_out=None, causal_mask=None):\n",
    "        # 1) self-attention enmascarado\n",
    "        h, self_w = self.self_attn(self.ln1(x), attn_mask=causal_mask)\n",
    "        x = x + self.dropout(h)\n",
    "\n",
    "        # 2) cross-attention opcional (caso encoder-decoder)\n",
    "        cross_w = None\n",
    "        if encoder_out is not None:\n",
    "            h, cross_w = self.cross_attn(self.ln_cross(x), x_kv=encoder_out, attn_mask=None)\n",
    "            x = x + self.dropout(h)\n",
    "\n",
    "        # 3) FFN\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x, self_w, cross_w\n",
    "\n",
    "# Pruebas de forma\n",
    "B, Tsrc, Ttgt, D = 2, 7, 6, 64\n",
    "enc_x = torch.randn(B, Tsrc, D)\n",
    "dec_x = torch.randn(B, Ttgt, D)\n",
    "\n",
    "enc_block = EncoderBlock(D, n_heads=4, d_ff=128)\n",
    "dec_block = DecoderBlock(D, n_heads=4, d_ff=128, use_rope=True, use_alibi=False)\n",
    "\n",
    "enc_out, enc_attn = enc_block(enc_x)\n",
    "dec_out, self_w, cross_w = dec_block(dec_x, encoder_out=enc_out, causal_mask=make_causal_mask(Ttgt))\n",
    "\n",
    "print(\"Salida del encoder:\", enc_out.shape)\n",
    "print(\"Salida del decoder:\", dec_out.shape)\n",
    "print(\"Self-attention del decoder:\", self_w.shape)\n",
    "print(\"Cross-attention del decoder:\", cross_w.shape if cross_w is not None else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62055e0f",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 5 - Encoder, decoder y cross-attention**\n",
    "\n",
    "1. Compara **encoder-only** vs. **decoder-only** en términos de:\n",
    "   - objetivo de entrenamiento,\n",
    "   - tipo de máscara,\n",
    "   - tipo de tarea.\n",
    "\n",
    "2. Explica qué hace el **cross-attention** en una arquitectura encoder/decoder.\n",
    "   ¿Qué parte produce Q y qué parte produce K/V?\n",
    "\n",
    "3. Relaciona cada variante con tareas típicas:\n",
    "   - clasificación/tagging\n",
    "   - traducción\n",
    "   - generación autoregresiva (LLM)\n",
    "\n",
    "4. ¿Por qué los LLMs modernos suelen usar decoder-only aunque otras variantes existan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63249c1",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 5**\n",
    "\n",
    "- **Encoder-only vs. Decoder-only:**  \n",
    "- **Cross-attention:**  \n",
    "- **Tareas por variante:**  \n",
    "- **Razón de uso en LLMs:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba04859",
   "metadata": {},
   "source": [
    "\n",
    "##### **5.1 Variante decoder-only (LLM) para next-token prediction**\n",
    "\n",
    "Construimos un modelo pequeño autoregresivo (solo **decoder**) para ilustrar:\n",
    "\n",
    "- embeddings de token\n",
    "- estrategia posicional (sin/cos o learned)\n",
    "- pila de bloques decoder\n",
    "- proyección final a vocabulario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f56025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDecoderOnlyLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        d_ff: int = 128,\n",
    "        n_layers: int = 2,\n",
    "        max_len: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "        pos_mode: str = \"sin\",   # \"sin\" o \"learned\"\n",
    "        use_rope: bool = True,\n",
    "        use_alibi: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pos_mode = pos_mode\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        if pos_mode == \"sin\":\n",
    "            self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len=max_len)\n",
    "        elif pos_mode == \"learned\":\n",
    "            self.pos_enc = LearnedPositionalEncoding(d_model, max_len=max_len)\n",
    "        else:\n",
    "            raise ValueError(\"pos_mode debe ser 'sin' o 'learned'\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, n_heads, d_ff, dropout=dropout, use_rope=use_rope, use_alibi=use_alibi)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor):\n",
    "        # idx: [B, T]\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.max_len, \"Secuencia demasiado larga para el max_len configurado\"\n",
    "\n",
    "        x = self.token_emb(idx)              # [B,T,D]\n",
    "        x = self.pos_enc(x)                  # [B,T,D]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        causal = make_causal_mask(T, device=idx.device)\n",
    "        last_attn = None\n",
    "        for block in self.blocks:\n",
    "            x, self_w, _ = block(x, encoder_out=None, causal_mask=causal)\n",
    "            last_attn = self_w\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                # [B,T,V]\n",
    "        return logits, last_attn\n",
    "\n",
    "# Prueba rápida\n",
    "model_test = TinyDecoderOnlyLM(vocab_size=vocab_size, pos_mode=\"sin\", use_rope=True, use_alibi=False).to(device)\n",
    "batch_ids = torch.tensor([\n",
    "    vocab.encode(\"the llama runs fast\"),\n",
    "    vocab.encode(\"the dog barks loudly\")\n",
    "], dtype=torch.long, device=device)\n",
    "logits, attn_last = model_test(batch_ids)\n",
    "print(\"Lote de entrada:\", batch_ids.shape)\n",
    "print(\"Logits:\", logits.shape)          # [B,T,V]\n",
    "print(\"Última atención:\", attn_last.shape if attn_last is not None else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdeb26",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 6 - Diseño del modelo decoder-only**\n",
    "\n",
    "1. Identifica los componentes mínimos de un modelo autoregresivo tipo LLM en este cuaderno.\n",
    "\n",
    "2. ¿Por qué el modelo aplica:\n",
    "   - embedding de token,\n",
    "   - estrategia posicional,\n",
    "   - bloques decoder,\n",
    "   - LayerNorm final,\n",
    "   - proyección a vocabulario?\n",
    "\n",
    "3. Explica qué representa la salida `logits` con forma `[B, T, V]`.\n",
    "   ¿Qué dimensión se usa para predecir el siguiente token?\n",
    "\n",
    "4. ¿Qué diferencia conceptual hay entre \"tener logits\" y \"tener probabilidades\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2a571",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 6**\n",
    "\n",
    "- **Componentes mínimos del decoder-only:**  \n",
    "- **Rol de cada etapa:**  \n",
    "- **Interpretación de logits [B,T,V]:**  \n",
    "- **Logits vs. probabilidades:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bc7ac",
   "metadata": {},
   "source": [
    "#### **6.  Mini entrenamiento**\n",
    "\n",
    "Usamos un `Dataset` y `DataLoader` personalizados. \n",
    "\n",
    "Usualmente el objetivo de entrenamiento:\n",
    "\n",
    "- **Next-token prediction** (modelo autoregresivo)\n",
    "- `CrossEntropyLoss` con `label_smoothing`\n",
    "- `AdamW`\n",
    "- Warmup + cosine decay\n",
    "- Gradient clipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], vocab: SimpleVocab, block_size: int = 12):\n",
    "        self.examples = []\n",
    "        self.block_size = block_size\n",
    "        for txt in texts:\n",
    "            ids = vocab.encode(txt, add_bos=True, add_eos=True)\n",
    "            # Ventanas deslizantes (corpus pequeño, basta 1-2 ventanas por línea)\n",
    "            if len(ids) < 3:\n",
    "                continue\n",
    "            for start in range(0, max(1, len(ids) - 2), max(1, block_size // 2)):\n",
    "                chunk = ids[start:start + block_size]\n",
    "                if len(chunk) < 3:\n",
    "                    continue\n",
    "                self.examples.append(chunk)\n",
    "\n",
    "        self.pad_id = vocab.pad_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.examples[idx]\n",
    "        x = ids[:-1]   # tokens de entrada\n",
    "        y = ids[1:]    # objetivos del siguiente token\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def collate_pad(batch, pad_id: int):\n",
    "    xs, ys = zip(*batch)\n",
    "    max_len = max(x.size(0) for x in xs)\n",
    "    xb = torch.full((len(xs), max_len), pad_id, dtype=torch.long)\n",
    "    yb = torch.full((len(ys), max_len), pad_id, dtype=torch.long)\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        xb[i, :x.size(0)] = x\n",
    "        yb[i, :y.size(0)] = y\n",
    "    return xb, yb\n",
    "\n",
    "dataset = NextTokenDataset(corpus, vocab, block_size=10)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True,\n",
    "                    collate_fn=lambda b: collate_pad(b, vocab.pad_id))\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "print(\"Tamaño del dataset:\", len(dataset))\n",
    "print(\"Lote x:\", xb.shape, \"Lote y:\", yb.shape)\n",
    "print(\"Ejemplo de ids x:\", xb[0].tolist())\n",
    "print(\"Ejemplo de ids y:\", yb[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c52fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo\n",
    "train_model = TinyDecoderOnlyLM(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=96,\n",
    "    n_heads=4,\n",
    "    d_ff=192,\n",
    "    n_layers=2,\n",
    "    max_len=32,\n",
    "    dropout=0.1,\n",
    "    pos_mode=\"sin\",      # probar \"learned\"\n",
    "    use_rope=True,       # RoPE dentro del self-attention\n",
    "    use_alibi=False      # poner True para probar ALiBi\n",
    ").to(device)\n",
    "\n",
    "# Pérdida: ignorar padding y añadir label smoothing\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_id, label_smoothing=0.1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(train_model.parameters(), lr=3e-3, weight_decay=0.01)\n",
    "\n",
    "# Calentamiento + decaimiento coseno\n",
    "num_epochs = 30\n",
    "steps_per_epoch = len(loader)\n",
    "total_steps = num_epochs * steps_per_epoch\n",
    "warmup_steps = max(5, int(0.1 * total_steps))\n",
    "\n",
    "def lr_lambda(step: int):\n",
    "    if step < warmup_steps:\n",
    "        return float(step + 1) / float(warmup_steps)\n",
    "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "print(f\"Pasos totales={total_steps}, pasos de calentamiento={warmup_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebeca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle de entrenamiento (demostración pequeña)\n",
    "train_model.train()\n",
    "loss_history = []\n",
    "lr_history = []\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    tokens_count = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits, _ = train_model(xb)  # [B,T,V]\n",
    "\n",
    "        # Aplanar para CE\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B * T, V), yb.view(B * T))\n",
    "\n",
    "        loss.backward()\n",
    "        # Recorte de gradiente para estabilizar el entrenamiento\n",
    "        nn.utils.clip_grad_norm_(train_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "        epoch_loss += loss.item()\n",
    "        lr_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # Contar objetivos válidos (no-pad) para un reporte aproximado\n",
    "        tokens_count += (yb != vocab.pad_id).sum().item()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        print(f\"Epoca {epoch:02d} | avg_loss={avg_loss:.4f} | lr={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Pérdida de entrenamiento\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Entropía cruzada\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(lr_history)\n",
    "plt.title(\"Tasa de aprendizaje (calentamiento + coseno)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616a275",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 7 - Entrenamiento y optimización**\n",
    "\n",
    "1. Explica el propósito de cada elemento del entrenamiento:\n",
    "   - `CrossEntropyLoss`\n",
    "   - `label_smoothing`\n",
    "   - `AdamW`\n",
    "   - warmup\n",
    "   - cosine decay\n",
    "   - gradient clipping\n",
    "\n",
    "2. ¿Por qué en `CrossEntropyLoss` se ignoran posiciones con `<pad>`?\n",
    "\n",
    "3. Interpreta las curvas de:\n",
    "   - pérdida de entrenamiento,\n",
    "   - tasa de aprendizaje.\n",
    "\n",
    "   ¿Qué comportamiento esperarías al inicio (warmup) y después?\n",
    "\n",
    "4. Si el entrenamiento fuera inestable (NaNs o pérdida explosiva), ¿qué revisarías primero y por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fdb67",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 7**\n",
    "\n",
    "- **Rol de los componentes de optimización:**  \n",
    "- **Padding e ignore_index:**  \n",
    "- **Interpretación de curvas:**  \n",
    "- **Plan de diagnóstico de inestabilidad:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f200c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extensión opcional (estudiante)\n",
    "# Ejecuta 2 experimentos y compara resultados:\n",
    "# 1) pos_mode=\"sin\" vs. pos_mode=\"learned\"\n",
    "# 2) use_rope=True vs. use_rope=False\n",
    "# 3) use_alibi=True vs. use_alibi=False\n",
    "#\n",
    "# Reporta luego (en texto):\n",
    "# - pérdida final aproximada\n",
    "# - estabilidad\n",
    "# - observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7652480",
   "metadata": {},
   "source": [
    "#### **7. Decoding: temperatura, top-k y top-p (nucleus)**\n",
    "\n",
    "Aunque el foco del cuaderno es el **bloque Transformer**, añadimos una celda de generación para conectar con el flujo de LLM:\n",
    "\n",
    "- **temperatura**: ajusta la entropía de la distribución\n",
    "- **top-k**: limita a los k tokens más probables\n",
    "- **top-p**: limita al menor conjunto con masa acumulada >= p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b283cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_next_token(logits_1d, temperature=1.0, top_k=None, top_p=None):\n",
    "    logits = logits_1d / max(1e-6, temperature)\n",
    "\n",
    "    if top_k is not None:\n",
    "        k = min(top_k, logits.size(-1))\n",
    "        values, _ = torch.topk(logits, k=k)\n",
    "        thresh = values[..., -1].unsqueeze(-1)\n",
    "        logits = torch.where(logits < thresh, torch.full_like(logits, float(\"-inf\")), logits)\n",
    "\n",
    "    if top_p is not None:\n",
    "        sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "        probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cumprobs = torch.cumsum(probs, dim=-1)\n",
    "        # Eliminar tokens después del umbral\n",
    "        sorted_mask = cumprobs > top_p\n",
    "        sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "        sorted_mask[..., 0] = False\n",
    "        sorted_logits[sorted_mask] = float(\"-inf\")\n",
    "        # Reubicar en el orden original\n",
    "        logits = torch.full_like(logits, float(\"-inf\"))\n",
    "        logits.scatter_(dim=-1, index=sorted_idx, src=sorted_logits)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, prompt: str, max_new_tokens=12, temperature=1.0, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    ids = vocab.encode(prompt, add_bos=True, add_eos=False)\n",
    "    idx = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if idx.size(1) > model.max_len:\n",
    "            idx_cond = idx[:, -model.max_len:]\n",
    "        else:\n",
    "            idx_cond = idx\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        next_logits = logits[0, -1]\n",
    "        next_id = sample_next_token(next_logits, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "        idx = torch.cat([idx, next_id.view(1, 1)], dim=1)\n",
    "\n",
    "        if next_id.item() == vocab.eos_id:\n",
    "            break\n",
    "\n",
    "    return vocab.decode(idx[0].tolist())\n",
    "\n",
    "for cfg in [\n",
    "    {\"temperature\": 1.0, \"top_k\": None, \"top_p\": None},\n",
    "    {\"temperature\": 0.8, \"top_k\": 5, \"top_p\": None},\n",
    "    {\"temperature\": 1.0, \"top_k\": None, \"top_p\": 0.9},\n",
    "]:\n",
    "    out = generate_text(train_model, prompt=\"the llama\", max_new_tokens=8, **cfg)\n",
    "    print(cfg, \"->\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2074356",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 8 - Decodificación y control de generación**\n",
    "\n",
    "1. Explica el efecto de la **temperatura** sobre la distribución de probabilidad.\n",
    "\n",
    "2. Compara **top-k** vs. **top-p**:\n",
    "   - ¿qué restringe cada uno?\n",
    "   - ¿cuándo uno puede ser más flexible que el otro?\n",
    "\n",
    "3. ¿Por qué una estrategia de muestreo puede producir resultados diferentes aun con el mismo prompt?\n",
    "\n",
    "4. Propón una configuración razonable (temperatura/top-k/top-p) para:\n",
    "   - salida más conservadora\n",
    "   - salida más diversa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c8edc",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 8**\n",
    "\n",
    "- **Temperatura:**  \n",
    "- **Top-k vs. Top-p:**  \n",
    "- **Variabilidad de la generación:**  \n",
    "- **Configuraciones propuestas:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c5864",
   "metadata": {},
   "source": [
    "#### **8. Variante encoder-only (MLM simplificado, conceptual)**\n",
    "\n",
    "Para cerrar, mostramos un esqueleto *encoder-only* (estilo BERT) que produce representaciones contextualizadas y una cabecera de clasificación por token (útil para **MLM**, tagging, etc.).  \n",
    "No entrenaremos MLM completo aquí; el objetivo es mostrar **la diferencia estructural** respecto al decoder-only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b49765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyEncoderOnly(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, n_heads=4, d_ff=128, n_layers=2, max_len=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = LearnedPositionalEncoding(d_model, max_len=max_len)  # encoder-only suele usar posiciones absolutas aprendidas\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.token_head = nn.Linear(d_model, vocab_size)  # cabecera por token (por ejemplo, MLM)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.token_emb(idx)\n",
    "        x = self.pos_enc(x)\n",
    "        attn_last = None\n",
    "        for blk in self.blocks:\n",
    "            x, attn_last = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.token_head(x)\n",
    "        return logits, attn_last\n",
    "\n",
    "enc_model = TinyEncoderOnly(vocab_size=vocab_size).to(device)\n",
    "enc_logits, enc_attn = enc_model(batch_ids)\n",
    "print(\"Logits encoder-only:\", enc_logits.shape)\n",
    "print(\"Atención encoder-only:\", enc_attn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f80bf",
   "metadata": {},
   "source": [
    "##### **Actividad guiada 9 - Comparación final de paradigmas**\n",
    "\n",
    "Elabora una síntesis final (máximo 12 líneas) que compare:\n",
    "\n",
    "- **Encoder-only**\n",
    "- **Decoder-only**\n",
    "- **Encoder/Decoder**\n",
    "\n",
    "Tu síntesis debe incluir:\n",
    "1. tipo de atención / máscara,\n",
    "2. objetivo de entrenamiento típico,\n",
    "3. tareas frecuentes,\n",
    "4. una ventaja y una limitación de cada enfoque.\n",
    "\n",
    "> Sugerencia: puedes responder en formato tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e254a1",
   "metadata": {},
   "source": [
    "##### **Respuesta del estudiante - Actividad 9**\n",
    "\n",
    "| Variante | Atención / máscara | Objetivo típico | Tareas | Ventaja | Limitación |\n",
    "|---|---|---|---|---|---|\n",
    "| Encoder-only |  |  |  |  |  |\n",
    "| Decoder-only |  |  |  |  |  |\n",
    "| Encoder/Decoder |  |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbace05-3d6e-44c9-9428-c045208daf0a",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "#### **1.  Visualizar atención con y sin máscara causal**\n",
    "\n",
    "Compara cómo cambia el mapa de atención cuando se activa/desactiva la máscara causal.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "1. Reutiliza `scaled_dot_product_attention`, `make_causal_mask` y tensores `q, k, v` (o créalos de nuevo).\n",
    "2. Calcula atención:\n",
    "\n",
    "   * **sin máscara**\n",
    "   * **con máscara causal**\n",
    "3. Grafica ambas matrices de atención para la **cabecera 0**.\n",
    "4. Escribe una observación breve en comentario dentro del código.\n",
    "\n",
    "**Qué debe observarse**\n",
    "\n",
    "* Sin máscara, cada posición puede atender a toda la secuencia.\n",
    "* Con máscara causal, las posiciones futuras quedan bloqueadas.\n",
    "\n",
    "#### **2. Comparar codificación posicional sinusoidal vs. aprendida**\n",
    "\n",
    "Observa diferencias en forma y activación entre dos estrategias de posición.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "1. Crea un tensor `x` de ceros con forma `[1, T, D]`.\n",
    "2. Aplícale:\n",
    "\n",
    "   * `SinusoidalPositionalEncoding`\n",
    "   * `LearnedPositionalEncoding`\n",
    "3. Grafica ambos resultados con `imshow`.\n",
    "4. Calcula una métrica sencilla de comparación (por ejemplo, norma o media).\n",
    "\n",
    "**Sugerencia**\n",
    "\n",
    "* Usa `T=64`, `D=32`.\n",
    "* Compara visualmente patrones regulares (sinusoidal) vs. patrones aprendidos.\n",
    "\n",
    "#### **3. Activar RoPE y ALiBi en el bloque de atención**\n",
    "\n",
    "Ejecuta el mismo bloque con distintas configuraciones posicionales modernas.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "1. Crea un lote aleatorio `x_demo` de forma `[B, T, D]`.\n",
    "2. Ejecuta tres variantes del bloque:\n",
    "\n",
    "   * **base** (`use_rope=False`, `use_alibi=False`)\n",
    "   * **con RoPE**\n",
    "   * **con ALiBi**\n",
    "3. Imprime formas de salida y atención.\n",
    "4. Grafica la atención de la **cabecera 0** para cada variante.\n",
    "\n",
    "**Qué comparar**\n",
    "\n",
    "* Forma del mapa de atención.\n",
    "* Cambios visuales entre variantes.\n",
    "* Si se mantiene la forma `[B,H,T,T]`.\n",
    "\n",
    "#### **4. Mini experimento de entrenamiento (sinusoidal vs learned)**\n",
    "\n",
    "Compara la pérdida de entrenamiento con dos estrategias posicionales.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "1. Reutiliza `NextTokenDataset`, `DataLoader` y `TinyDecoderOnlyLM`.\n",
    "2. Entrena **dos modelos pequeños** (5-10 épocas):\n",
    "\n",
    "   * `pos_mode=\"sin\"`\n",
    "   * `pos_mode=\"learned\"`\n",
    "3. Guarda la pérdida promedio por época.\n",
    "4. Grafica ambas curvas de pérdida.\n",
    "5. Escribe una conclusión breve en comentario.\n",
    "\n",
    "**Qué reportar**\n",
    "\n",
    "* Pérdida final aproximada de cada variante.\n",
    "* Estabilidad observada.\n",
    "* Si alguna converge más rápido en este corpus pequeño.\n",
    "\n",
    "#### **5. Generación controlada (temperatura, top-k, top-p)**\n",
    "\n",
    "Compara el efecto de los hiperparámetros de decodificación.\n",
    "\n",
    "**Instrucciones**\n",
    "\n",
    "1. Reutiliza `generate_text`.\n",
    "2. Usa un mismo prompt (por ejemplo: `\"the llama\"` o `\"transformers use\"`).\n",
    "3. Genera texto con al menos **4 configuraciones**:\n",
    "\n",
    "   * base\n",
    "   * baja temperatura\n",
    "   * top-k\n",
    "   * top-p\n",
    "4. Imprime resultados y comenta cuál configuración produce salidas más conservadoras o más diversas.\n",
    "\n",
    "**Sugerencia de configuraciones**\n",
    "\n",
    "* `temperature=1.0`\n",
    "* `temperature=0.7`\n",
    "* `top_k=5`\n",
    "* `top_p=0.9`\n",
    "\n",
    "**Qué analizar**\n",
    "\n",
    "* Coherencia\n",
    "* Diversidad\n",
    "* Repetición\n",
    "* Sensibilidad a la aleatoriedad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4dd16-1a72-4484-827c-c21dd2dba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922cc87",
   "metadata": {},
   "source": [
    "#### **Resumen final**\n",
    "\n",
    "- El **bloque Transformer** combina:\n",
    "  - **Self-attention** (Q, K, V)\n",
    "  - **Multi-head attention**\n",
    "  - **Residual + LayerNorm**\n",
    "  - **FFN**\n",
    "- La complejidad de la atención densa escala como **$O(T^2)$** por la matriz de logits **T×T**.\n",
    "- **RoPE** y **ALiBi** son variantes modernas para codificar posición directamente en la atención.\n",
    "- En práctica:\n",
    "  - **encoder-only** -> representaciones/contexto global (BERT-like)\n",
    "  - **decoder-only** -> predicción autoregresiva (LLM)\n",
    "  - **encoder/decoder** -> condicionamiento por cross-attention (traducción, T5-like)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
