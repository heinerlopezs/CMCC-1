{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317216fc",
   "metadata": {},
   "source": [
    "### **LLMs causales, instruction tuning, alineamiento y agentes**\n",
    "\n",
    "Este cuaderno está **orientado a investigación aplicada** y cubre cuatro bloques integrados:\n",
    "\n",
    "1. **Modelos de lenguaje causales**: tokenización, ventana de contexto y efectos experimentales.  \n",
    "2. **Instruction tuning** (SFT): cómo ajustar un modelo base a formato instrucción-respuesta.  \n",
    "3. **Alineamiento**: ideas de **RLHF**, **DPO/ORPO** y modelos de preferencia.  \n",
    "4. **LLMs como agentes**: herramientas, memoria, planificación y evaluación del comportamiento.\n",
    "\n",
    "> **Nota:** algunas celdas son **demostraciones ligeras** (corren localmente), y otras son **plantillas** para ejecutar con GPU/Colab/servidores de laboratorio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d52edc",
   "metadata": {},
   "source": [
    "### **0. Configuración y reproducibilidad**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8053a7c",
   "metadata": {},
   "source": [
    "#### **0.1 Principios mínimos**\n",
    "En investigación con LLMs, documenta siempre:\n",
    "\n",
    "- **modelo base** (nombre y versión),\n",
    "- **tokenizer** (mismo repo del modelo),\n",
    "- **dataset** (origen + fecha + versión),\n",
    "- **prompt template**,\n",
    "- **semilla** (`seed`),\n",
    "- **hardware** (GPU/VRAM),\n",
    "- **métricas** y scripts de evaluación,\n",
    "- **criterio de selección** (mejor checkpoint por qué métrica).\n",
    "\n",
    "Esto evita resultados \"bonitos pero irreproducibles\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcf92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibilidad básica\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch is not None:\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Seed fija: {SEED}\")\n",
    "print(f\"Torch disponible: {torch is not None}\")\n",
    "if torch is not None:\n",
    "    print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbe5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registro sencillo de experimentos (sin depender de Weights & Biases)\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR = Path(\"logs_investigacion_llm\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def registrar_experimento(nombre: str, config: Dict[str, Any], resultados: Dict[str, Any]) -> str:\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = LOG_DIR / f\"{timestamp}_{nombre}.json\"\n",
    "    payload = {\n",
    "        \"timestamp_utc\": timestamp,\n",
    "        \"nombre\": nombre,\n",
    "        \"config\": config,\n",
    "        \"resultados\": resultados,\n",
    "    }\n",
    "    path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    return str(path)\n",
    "\n",
    "# Ejemplo rápido\n",
    "ruta_log = registrar_experimento(\n",
    "    \"demo_setup\",\n",
    "    {\"seed\": SEED, \"objetivo\": \"verificar formato de logs\"},\n",
    "    {\"status\": \"ok\"}\n",
    ")\n",
    "print(\"Log guardado en:\", ruta_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79599d23",
   "metadata": {},
   "source": [
    "### **1. Modelos de lenguaje causales, tokenización y ventana de contexto**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e064574",
   "metadata": {},
   "source": [
    "#### **1.1 Modelo causal (decoder-only)**\n",
    "Un **LLM causal** predice el siguiente token:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, ..., x_T)=\\prod_{t=1}^{T} p(x_t \\mid x_{<t})\n",
    "$$\n",
    "\n",
    "Esto lo hace ideal para:\n",
    "- generación de texto,\n",
    "- chat,\n",
    "- código,\n",
    "- planificación textual (agents),\n",
    "- tool-use via prompts o llamadas estructuradas.\n",
    "\n",
    "##### **Pregunta de investigación**\n",
    "\n",
    "¿Cómo cambia el rendimiento cuando:\n",
    "- varía la **tokenización**,\n",
    "- aumenta/disminuye la **ventana de contexto**,\n",
    "- cambia el formato del prompt?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010368d8",
   "metadata": {},
   "source": [
    "#### **1.2 Tokenización (demostración ligera)**\n",
    "Primero usamos una tokenización simple para visualizar conceptos. Luego mostramos cómo hacerlo con **Hugging Face** (si tienes acceso al modelo/tokenizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización muy simple\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize_simple(texto: str) -> List[str]:\n",
    "    # Separa palabras y signos de puntuación de forma básica\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", texto, re.UNICODE)\n",
    "\n",
    "texto = \"Los modelos causales predicen el siguiente token. ¡Eso afecta la ventana de contexto!\"\n",
    "tokens = tokenize_simple(texto)\n",
    "\n",
    "print(\"Texto:\", texto)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"N° tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación rápida entre textos de distinta granularidad\n",
    "textos = [\n",
    "    \"Hola mundo.\",\n",
    "    \"Los LLMs pueden razonar mejor con prompts estructurados.\",\n",
    "    \"def suma(a, b): return a + b\"\n",
    "]\n",
    "\n",
    "for t in textos:\n",
    "    toks = tokenize_simple(t)\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Texto:\", t)\n",
    "    print(\"Tokens:\", toks)\n",
    "    print(\"Longitud:\", len(toks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ed01e",
   "metadata": {},
   "source": [
    "#### **1.3 Tokenización con Hugging Face (opcional)**\n",
    "Esta celda usa `transformers` y un tokenizer real. Si no estás en Colab o no tienes internet, puedes saltarla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85982c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización real con Hugging Face (opcional)\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer_name = \"gpt2\"  # cambia por un tokenizer de tu modelo base\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    muestras = [\n",
    "        \"La tokenización subword afecta longitud, costo y truncamiento.\",\n",
    "        \"Instruction tuning requiere un template consistente.\",\n",
    "        \"¿Qué pasa con DPO/ORPO cuando el prompt es ambiguo?\"\n",
    "    ]\n",
    "\n",
    "    for s in muestras:\n",
    "        ids = hf_tokenizer.encode(s, add_special_tokens=False)\n",
    "        toks = hf_tokenizer.convert_ids_to_tokens(ids)\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Texto:\", s)\n",
    "        print(\"IDs:\", ids)\n",
    "        print(\"Tokens:\", toks)\n",
    "        print(\"N° tokens:\", len(ids))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo ejecutar la tokenización Hugging Face (opcional).\")\n",
    "    print(\"Detalle:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3542fb",
   "metadata": {},
   "source": [
    "#### **1.4 Ventana de contexto: truncamiento vs sliding window**\n",
    "En investigación, una fuente común de errores es **truncar silenciosamente**.  \n",
    "Hay que medir:\n",
    "\n",
    "- cuántos ejemplos fueron truncados,\n",
    "- cuánto contenido se perdió,\n",
    "- si el truncamiento sesga ciertas clases/tipos de prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de ventana de contexto\n",
    "from typing import Iterable\n",
    "\n",
    "def truncar_tokens(tokens: List[str], max_len: int) -> List[str]:\n",
    "    return tokens[:max_len]\n",
    "\n",
    "def sliding_windows(tokens: List[str], window: int, stride: int) -> List[List[str]]:\n",
    "    ventanas = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i+window]\n",
    "        if not chunk:\n",
    "            break\n",
    "        ventanas.append(chunk)\n",
    "        if i + window >= len(tokens):\n",
    "            break\n",
    "        i += stride\n",
    "    return ventanas\n",
    "\n",
    "texto_largo = \" \".join([\"contexto\"] * 45) + \" fin\"\n",
    "tokens_largos = tokenize_simple(texto_largo)\n",
    "\n",
    "print(\"Total tokens:\", len(tokens_largos))\n",
    "print(\"Truncado a 16:\", truncar_tokens(tokens_largos, 16))\n",
    "\n",
    "ventanas = sliding_windows(tokens_largos, window=16, stride=8)\n",
    "print(\"\\nN° ventanas:\", len(ventanas))\n",
    "for i, v in enumerate(ventanas[:3]):\n",
    "    print(f\"Ventana {i}: len={len(v)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8edb3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de truncamiento sobre un mini-corpus\n",
    "corpus = [\n",
    "    \" \".join([\"a\"] * 8),\n",
    "    \" \".join([\"b\"] * 20),\n",
    "    \" \".join([\"c\"] * 32),\n",
    "    \" \".join([\"d\"] * 70),\n",
    "]\n",
    "\n",
    "MAX_LEN = 16\n",
    "stats = []\n",
    "for i, txt in enumerate(corpus):\n",
    "    toks = tokenize_simple(txt)\n",
    "    n = len(toks)\n",
    "    truncados = max(0, n - MAX_LEN)\n",
    "    stats.append({\n",
    "        \"ejemplo\": i,\n",
    "        \"tokens\": n,\n",
    "        \"truncados\": truncados,\n",
    "        \"fraccion_truncada\": truncados / n if n else 0.0,\n",
    "    })\n",
    "\n",
    "for row in stats:\n",
    "    print(row)\n",
    "\n",
    "prom_trunc = sum(r[\"fraccion_truncada\"] for r in stats) / len(stats)\n",
    "print(\"\\nFracción truncada promedio:\", round(prom_trunc, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b40329",
   "metadata": {},
   "source": [
    "#### **1.5 Demostración de un pequeño modelo causal**\n",
    "Esta demostración no reemplaza a un LLM real, pero ayuda a entender la lógica causal:\n",
    "- entrada: secuencia\n",
    "- objetivo: **siguiente token**\n",
    "- pérdida: `CrossEntropy` desplazando en 1 posición\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo causal a nivel de caracteres\n",
    "# Nota: se entrena sobre texto pequeño, solo para entender el flujo.\n",
    "if torch is None:\n",
    "    print(\"Torch no disponible; salta esta celda.\")\n",
    "else:\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    texto_toy = \"los llm causales predicen el siguiente token. \" * 30\n",
    "    vocab = sorted(list(set(texto_toy)))\n",
    "    stoi = {ch:i for i,ch in enumerate(vocab)}\n",
    "    itos = {i:ch for ch,i in stoi.items()}\n",
    "    data = torch.tensor([stoi[ch] for ch in texto_toy], dtype=torch.long)\n",
    "\n",
    "    block_size = 24\n",
    "    def batch_toy(batch_size=16):\n",
    "        ix = torch.randint(0, len(data)-block_size-1, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        return x, y\n",
    "\n",
    "    class MiniCausalLM(nn.Module):\n",
    "        def __init__(self, vocab_size, d_model=64):\n",
    "            super().__init__()\n",
    "            self.emb = nn.Embedding(vocab_size, d_model)\n",
    "            self.pos = nn.Embedding(block_size, d_model)\n",
    "            self.ln = nn.LayerNorm(d_model)\n",
    "            self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        def forward(self, x, y=None):\n",
    "            B, T = x.shape\n",
    "            tok = self.emb(x)\n",
    "            pos = self.pos(torch.arange(T, device=x.device))[None, :, :]\n",
    "            h = self.ln(tok + pos)\n",
    "            logits = self.head(h)  # [B, T, V]\n",
    "            loss = None\n",
    "            if y is not None:\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    model = MiniCausalLM(vocab_size=len(vocab))\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "    model.train()\n",
    "    for step in range(120):\n",
    "        xb, yb = batch_toy()\n",
    "        logits, loss = model(xb, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if step % 30 == 0:\n",
    "            print(f\"step={step:03d} loss={loss.item():.4f}\")\n",
    "\n",
    "    # Generación autoregresiva simple\n",
    "    model.eval()\n",
    "    contexto = torch.tensor([[stoi[c] for c in \"los llm \"]], dtype=torch.long)\n",
    "    out = contexto.clone()\n",
    "\n",
    "    for _ in range(60):\n",
    "        x = out[:, -block_size:]\n",
    "        logits, _ = model(x)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        out = torch.cat([out, next_id], dim=1)\n",
    "\n",
    "    generado = \"\".join(itos[i] for i in out[0].tolist())\n",
    "    print(\"\\nTexto generado:\")\n",
    "    print(generado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02866bf8",
   "metadata": {},
   "source": [
    "#### **1.6 Métricas relevantes para este bloque**\n",
    "Para reportes de investigación:\n",
    "\n",
    "- **Longitud en tokens** (prompt, respuesta, total)\n",
    "- **% truncado**\n",
    "- **Costo por ejemplo** (tokens procesados)\n",
    "- **Perplejidad / NLL** (si aplica)\n",
    "- **Latencia de generación**\n",
    "- **Tasa de errores de formato** (si pides JSON/tool-call)\n",
    "\n",
    "> En muchos proyectos, mejorar tokenización/prompt/contexto da más ganancia que \"entrenar más\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25a99d",
   "metadata": {},
   "source": [
    "### **2. Instrucción y ajuste fino sobre modelos preentrenados (Instruction Tuning/SFT)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e31a4a",
   "metadata": {},
   "source": [
    "#### **2.1 Qué es instruction tuning**\n",
    "\n",
    "Partimos de un modelo base preentrenado (causal) y lo ajustamos con ejemplos tipo:\n",
    "\n",
    "- **instrucción** (qué se pide),\n",
    "- **entrada** opcional (contexto),\n",
    "- **salida** (respuesta deseada).\n",
    "\n",
    "Esto se suele llamar:\n",
    "- **SFT** (Supervised Fine-Tuning),\n",
    "- **instruction tuning**,\n",
    "- **instruct tuning**.\n",
    "\n",
    "##### **Hipótesis de investigación**\n",
    "Un mejor *template de prompt* y una mejor *curación de datos* pueden mejorar más que aumentar parámetros entrenables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62cdf3",
   "metadata": {},
   "source": [
    "#### **2.2 Formato de datos (estándar mínimo)**\n",
    "Usaremos un formato simple compatible con datasets JSONL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa328f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sintético de ejemplo (para estructura, no para calidad real)\n",
    "dataset_sft = [\n",
    "    {\n",
    "        \"instruction\": \"Resume el siguiente texto en una oración.\",\n",
    "        \"input\": \"Los transformadores usan atención para modelar dependencias de largo alcance.\",\n",
    "        \"output\": \"Los transformadores usan atención para capturar dependencias largas en secuencias.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Clasifica el sentimiento (positivo/negativo).\",\n",
    "        \"input\": \"El curso estuvo muy bien organizado y aprendí bastante.\",\n",
    "        \"output\": \"positivo\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Extrae una lista JSON de tecnologías mencionadas.\",\n",
    "        \"input\": \"Usamos PyTorch, Hugging Face, FastAPI y Docker en el proyecto.\",\n",
    "        \"output\": \"[\\\"PyTorch\\\", \\\"Hugging Face\\\", \\\"FastAPI\\\", \\\"Docker\\\"]\"\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_sft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template de prompt para modelo causal\n",
    "def formatear_prompt_sft(ej: Dict[str, str]) -> str:\n",
    "    inst = ej[\"instruction\"].strip()\n",
    "    inp = ej.get(\"input\", \"\").strip()\n",
    "    out = ej[\"output\"].strip()\n",
    "\n",
    "    if inp:\n",
    "        return (\n",
    "            \"### Instrucción:\\n\"\n",
    "            f\"{inst}\\n\\n\"\n",
    "            \"### Entrada:\\n\"\n",
    "            f\"{inp}\\n\\n\"\n",
    "            \"### Respuesta:\\n\"\n",
    "            f\"{out}\"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            \"### Instrucción:\\n\"\n",
    "            f\"{inst}\\n\\n\"\n",
    "            \"### Respuesta:\\n\"\n",
    "            f\"{out}\"\n",
    "        )\n",
    "\n",
    "for ej in dataset_sft:\n",
    "    print(\"=\" * 80)\n",
    "    print(formatear_prompt_sft(ej))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7762bc",
   "metadata": {},
   "source": [
    "#### **2.3 Preparación de etiquetas (masking)**\n",
    "En SFT con modelos causales, normalmente queremos que la pérdida se calcule **solo sobre la respuesta del asistente**, no sobre toda la instrucción.\n",
    "\n",
    "Eso se logra con **label masking**:\n",
    "- tokens del prompt -> `-100`\n",
    "- tokens de respuesta -> ids reales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración conceptual de label masking sin depender de HF\n",
    "def construir_labels_masked(tokens_prompt: List[int], tokens_respuesta: List[int]) -> Tuple[List[int], List[int]]:\n",
    "    input_ids = tokens_prompt + tokens_respuesta\n",
    "    labels = [-100] * len(tokens_prompt) + tokens_respuesta[:]  # solo aprende la respuesta\n",
    "    return input_ids, labels\n",
    "\n",
    "# Ejemplo con IDs ficticios\n",
    "prompt_ids = [10, 11, 12, 13]\n",
    "resp_ids = [50, 51, 52]\n",
    "input_ids, labels = construir_labels_masked(prompt_ids, resp_ids)\n",
    "\n",
    "print(\"input_ids:\", input_ids)\n",
    "print(\"labels   :\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614b644",
   "metadata": {},
   "source": [
    "#### **2.4 Plantilla de SFT con Hugging Face + TRL (PEFT/LoRA)**\n",
    "Esta sección es una **plantilla de investigación**.  \n",
    "Adáptala con tu modelo base y tu dataset real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla SFT con Transformers/TRL/PEFT (opcional, requiere dependencias y GPU)\n",
    "# Sugerido para Colab/A100/Lab con CUDA.\n",
    "\n",
    "# !pip install -U transformers datasets trl peft accelerate bitsandbytes\n",
    "\n",
    "sft_template = r'''\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"  # ejemplo (ajústalo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # device_map=\"auto\", torch_dtype=\"auto\"  # opcional según hardware\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # depende del modelo\n",
    ")\n",
    "\n",
    "# Si tu dataset está en JSONL con columnas instruction/input/output:\n",
    "# ds = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"})\n",
    "\n",
    "def format_example(example):\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = (example.get(\"input\") or \"\").strip()\n",
    "    output_text = example[\"output\"].strip()\n",
    "    if input_text:\n",
    "        return f\"\"\"### Instrucción:\n",
    "{instruction}\n",
    "\n",
    "### Entrada:\n",
    "{input_text}\n",
    "\n",
    "### Respuesta:\n",
    "{output_text}\"\"\"\n",
    "    return f\"\"\"### Instrucción:\n",
    "{instruction}\n",
    "\n",
    "### Respuesta:\n",
    "{output_text}\"\"\"\n",
    "\n",
    "# ds = ds.map(lambda x: {\"text\": format_example(x)})\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"outputs_sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    report_to=[],  # agrega \"wandb\" si lo usas\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds.get(\"validation\"),\n",
    "    args=cfg,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"outputs_sft/final\")\n",
    "'''\n",
    "print(sft_template[:2000])\n",
    "print(\"\\n... (plantilla completa en la variable sft_template)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7a910",
   "metadata": {},
   "source": [
    "#### **2.5 QLoRA (orientación de investigación)**\n",
    "Si el modelo es más grande y tu VRAM es limitada, usa **QLoRA**:\n",
    "\n",
    "- pesos base cuantizados (4-bit),\n",
    "- adaptadores LoRA entrenables,\n",
    "- costo menor de VRAM.\n",
    "\n",
    "##### Variables de investigación útiles\n",
    "- `r` (rank LoRA)\n",
    "- `target_modules`\n",
    "- `max_seq_length`\n",
    "- `packing`\n",
    "- mezcla de datasets\n",
    "- proporción de ejemplos largos/cortos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e666448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla conceptual de configuración QLoRA (no ejecuta por sí sola)\n",
    "qlora_config = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",  # o float16 según hardware\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}\n",
    "qlora_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fda69",
   "metadata": {},
   "source": [
    "#### **2.6 Evaluación de SFT (mínimo experimental)**\n",
    "No evalúes solo con \"se ve bien\". Usa al menos:\n",
    "\n",
    "- **Exact match / F1** (si la tarea lo permite),\n",
    "- **ROUGE / BLEU** (si es resumen o traducción, con cuidado),\n",
    "- **format compliance** (JSON válido, campos correctos),\n",
    "- **human eval pequeña** (rubrica),\n",
    "- **errores frecuentes** (halucinación, longitud, formato).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c07bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de formato JSOn\n",
    "import json\n",
    "\n",
    "predicciones = [\n",
    "    '{\"tecnologias\": [\"PyTorch\", \"Docker\"]}',\n",
    "    '{\"tecnologias\": [\"FastAPI\", \"Hugging Face\"]}',\n",
    "    'No es JSON'\n",
    "]\n",
    "\n",
    "def es_json_valido(s: str) -> bool:\n",
    "    try:\n",
    "        json.loads(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "validas = [es_json_valido(p) for p in predicciones]\n",
    "print(\"Predicciones válidas:\", validas)\n",
    "print(\"Tasa de formato válido:\", sum(validas) / len(validas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5048bec",
   "metadata": {},
   "source": [
    "### **3. Alineamiento: RLHF, DPO/ORPO y modelos de preferencia**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b332",
   "metadata": {},
   "source": [
    "#### **3.1 Panorama conceptual**\n",
    "#### **RLHF** (visión clásica)\n",
    "\n",
    "Pipeline típico:\n",
    "\n",
    "1. **SFT** (modelo instruccional base)  \n",
    "2. Recolectar **preferencias humanas** (A mejor que B)  \n",
    "3. Entrenar un **modelo de preferencia/recompensa**  \n",
    "4. Optimizar la política (PPO u otra técnica)\n",
    "\n",
    "#### **DPO/ORPO**\n",
    "\n",
    "Evitan parte de la complejidad de RLHF clásico:\n",
    "- **DPO** optimiza directamente con pares preferidos/rechazados.\n",
    "- **ORPO** integra una señal de preferencia junto con el objetivo supervisado.\n",
    "\n",
    "> En práctica, DPO/ORPO suelen ser más simples de reproducir que PPO-RLHF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16a079",
   "metadata": {},
   "source": [
    "#### **3.2 Esquema de datos de preferencia**\n",
    "\n",
    "Formato típico de ejemplo:\n",
    "\n",
    "- `prompt`\n",
    "- `chosen` (respuesta preferida)\n",
    "- `rejected` (respuesta menos preferida)\n",
    "\n",
    "La calidad del dataset de preferencias es crítica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de preferencias\n",
    "dataset_pref = [\n",
    "    {\n",
    "        \"prompt\": \"Explica qué es una ventana de contexto en 2 líneas.\",\n",
    "        \"chosen\": \"La ventana de contexto es la cantidad de tokens que el modelo puede considerar a la vez. Si el texto excede ese límite, parte del contenido se trunca o se procesa por fragmentos.\",\n",
    "        \"rejected\": \"Es la memoria del modelo para siempre y nunca se pierde información.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Devuelve una lista JSON con 3 frutas.\",\n",
    "        \"chosen\": '[\"manzana\", \"pera\", \"uva\"]',\n",
    "        \"rejected\": \"manzana, pera, uva\"\n",
    "    },\n",
    "]\n",
    "dataset_pref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fa045",
   "metadata": {},
   "source": [
    "#### **3.3 Modelo de preferencia (reward/preference model)**\n",
    "\n",
    "En RLHF clásico, el reward model (modelo de recompensa) aprende a asignar un score a respuestas.  Aquí hacemos una demostración ligera con rasgos simples para entender el flujo (no para uso real).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f75706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración ligera: \"modelo de preferencia\" heurístico (no ML real)\n",
    "# Sirve para entender cómo se generan señales de preferencia.\n",
    "import math\n",
    "\n",
    "def score_respuesta_heuristico(prompt: str, respuesta: str) -> float:\n",
    "    score = 0.0\n",
    "    # Penaliza respuestas demasiado cortas\n",
    "    n = len(respuesta.split())\n",
    "    score += min(n / 15.0, 1.5)\n",
    "    # Bonifica formato JSON si el prompt lo pide\n",
    "    if \"json\" in prompt.lower():\n",
    "        try:\n",
    "            json.loads(respuesta)\n",
    "            score += 1.0\n",
    "        except Exception:\n",
    "            score -= 0.5\n",
    "    # Penaliza afirmaciones absolutas dudosas (ejemplo didáctico)\n",
    "    if \"nunca\" in respuesta.lower() and \"siempre\" in respuesta.lower():\n",
    "        score -= 0.3\n",
    "    return score\n",
    "\n",
    "for ej in dataset_pref:\n",
    "    s_ch = score_respuesta_heuristico(ej[\"prompt\"], ej[\"chosen\"])\n",
    "    s_rj = score_respuesta_heuristico(ej[\"prompt\"], ej[\"rejected\"])\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Prompt:\", ej[\"prompt\"])\n",
    "    print(\"Score chosen  :\", round(s_ch, 3))\n",
    "    print(\"Score rejected:\", round(s_rj, 3))\n",
    "    print(\"¿Chosen mejor?:\", s_ch > s_rj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a8a80",
   "metadata": {},
   "source": [
    "#### **3.4 Pérdida DPO (demostración matemática con log-probabilidades)**\n",
    "La idea central de DPO: aumentar la preferencia del modelo por `chosen` frente a `rejected`, comparado contra un modelo de referencia.\n",
    "\n",
    "Para entenderlo, simulamos log-probabilidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración de pérdida DPO con tensores (sin modelo real)\n",
    "if torch is None:\n",
    "    print(\"Torch no disponible; salta esta celda.\")\n",
    "else:\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # Log-probs simuladas por ejemplo (sumadas sobre la respuesta)\n",
    "    logp_pi_chosen = torch.tensor([ -5.2, -3.1, -7.8 ])\n",
    "    logp_pi_rej    = torch.tensor([ -6.1, -2.6, -8.7 ])\n",
    "    logp_ref_chosen= torch.tensor([ -5.5, -3.0, -7.9 ])\n",
    "    logp_ref_rej   = torch.tensor([ -5.9, -2.8, -8.5 ])\n",
    "\n",
    "    beta = 0.1\n",
    "\n",
    "    # Δπ y Δref\n",
    "    delta_pi = logp_pi_chosen - logp_pi_rej\n",
    "    delta_ref = logp_ref_chosen - logp_ref_rej\n",
    "\n",
    "    # Pérdida DPO: -log(sigmoid(beta * (Δπ - Δref)))\n",
    "    loss = -F.logsigmoid(beta * (delta_pi - delta_ref)).mean()\n",
    "\n",
    "    print(\"delta_pi :\", delta_pi)\n",
    "    print(\"delta_ref:\", delta_ref)\n",
    "    print(\"DPO loss :\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3330f0",
   "metadata": {},
   "source": [
    "#### **3.5 ORPO (idea + demo conceptual)**\n",
    "\n",
    "ORPO combina:\n",
    "- objetivo supervisado (NLL de la respuesta correcta),\n",
    "- una señal de preferencia (odds ratio/ranking).\n",
    "\n",
    "No implementaremos la fórmula completa aquí, pero sí una versión conceptual para ver el balance de términos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración conceptual de combinación SFT + preferencia (estilo ORPO simplificado)\n",
    "if torch is None:\n",
    "    print(\"Torch no disponible; salta esta celda.\")\n",
    "else:\n",
    "    # Simulamos dos términos:\n",
    "    nll_sft = torch.tensor(1.85)          # pérdida de lenguaje\n",
    "    pref_penalty = torch.tensor(0.42)     # término de preferencia (ejemplo)\n",
    "    alpha = 0.5                           # peso de la preferencia\n",
    "\n",
    "    loss_total = nll_sft + alpha * pref_penalty\n",
    "\n",
    "    print(\"NLL SFT         :\", float(nll_sft))\n",
    "    print(\"Término prefer. :\", float(pref_penalty))\n",
    "    print(\"alpha           :\", alpha)\n",
    "    print(\"Loss total      :\", float(loss_total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f5dd0",
   "metadata": {},
   "source": [
    "#### **3.6 Plantillas DPO/ORPO con TRL (investigación)**\n",
    "Estas son plantillas de alto nivel para adaptar a tu entorno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e65ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla DPO con TRL (opcional)\n",
    "dpo_template = r'''\n",
    "# !pip install -U transformers datasets trl peft accelerate bitsandbytes\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"  # ejemplo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Dataset con columnas: prompt, chosen, rejected\n",
    "# ds = load_dataset(\"json\", data_files={\"train\": \"pref_train.jsonl\", \"validation\": \"pref_val.jsonl\"})\n",
    "\n",
    "cfg = DPOConfig(\n",
    "    output_dir=\"outputs_dpo\",\n",
    "    beta=0.1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=cfg,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds.get(\"validation\"),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"outputs_dpo/final\")\n",
    "'''\n",
    "print(dpo_template[:1800])\n",
    "print(\"\\n... (plantilla completa en la variable dpo_template)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600bbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla ORPO con TRL (opcional, si tu versión de TRL lo soporta)\n",
    "orpo_template = r'''\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import ORPOTrainer, ORPOConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "cfg = ORPOConfig(\n",
    "    output_dir=\"outputs_orpo\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    report_to=[],\n",
    "    # beta / otros parámetros dependen de la versión\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=cfg,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],      # columnas: prompt, chosen, rejected\n",
    "    eval_dataset=ds.get(\"validation\"),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"outputs_orpo/final\")\n",
    "'''\n",
    "print(orpo_template[:1800])\n",
    "print(\"\\n... (plantilla completa en la variable orpo_template)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8488634",
   "metadata": {},
   "source": [
    "#### **3.7 Qué medir en alineamiento**\n",
    "\n",
    "Además de métricas de tarea, mide:\n",
    "\n",
    "- **Win rate** en pares de preferencia (vs baseline)\n",
    "- **Format compliance** (muy importante)\n",
    "- **Toxicidad / seguridad** (si aplica)\n",
    "- **Tasa de rechazo apropiado** (cuando la petición es riesgosa o inválida)\n",
    "- **Over-optimization** (respuestas demasiado largas/complacientes)\n",
    "- **Robustez a prompts ambiguos**\n",
    "\n",
    "##### **Riesgo común**\n",
    "Optimizar solo por \"preferencia\" puede degradar **exactitud factual**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587625d",
   "metadata": {},
   "source": [
    "### **4. LLM como agentes: herramientas, memoria y planificación**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44482e89",
   "metadata": {},
   "source": [
    "#### **4.1 Qué entendemos por \"agente\"**\n",
    "En este cuaderno, un agente es un sistema que combina:\n",
    "\n",
    "- un **modelo de lenguaje** (razonamiento/generación),\n",
    "- **herramientas** (calculadora, búsqueda, base de datos, código),\n",
    "- **memoria** (estado de conversación, hechos, resultados previos),\n",
    "- **planificación** (dividir tareas, decidir siguiente acción).\n",
    "\n",
    "> Investigación real aquí = medir si el uso de herramientas mejora la tarea, no solo demostrar \"que llama una función\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b86e7",
   "metadata": {},
   "source": [
    "#### **4.2 Herramientas**\n",
    "Primero creamos herramientas locales y observables (con trazas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62100b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herramientas con trazas\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    nombre: str\n",
    "    entrada: Dict[str, Any]\n",
    "    salida: Any\n",
    "\n",
    "@dataclass\n",
    "class TrazaAgente:\n",
    "    pasos: List[str] = field(default_factory=list)\n",
    "    tool_calls: List[ToolCall] = field(default_factory=list)\n",
    "\n",
    "    def log(self, msg: str):\n",
    "        self.pasos.append(msg)\n",
    "\n",
    "    def registrar_tool(self, nombre: str, entrada: Dict[str, Any], salida: Any):\n",
    "        self.tool_calls.append(ToolCall(nombre, entrada, salida))\n",
    "\n",
    "def tool_calculadora(expr: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        # Demo controlada: eval local básico\n",
    "        resultado = eval(expr, {\"__builtins__\": {}}, {})\n",
    "        return {\"ok\": True, \"resultado\": resultado}\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "\n",
    "BASE_KB = {\n",
    "    \"rlhf\": \"RLHF suele incluir SFT, modelo de preferencia y optimización de política.\",\n",
    "    \"dpo\": \"DPO optimiza preferencias con pares chosen/rejected sin PPO clásico.\",\n",
    "    \"orpo\": \"ORPO integra objetivo supervisado y señal de preferencia.\"\n",
    "}\n",
    "\n",
    "def tool_busqueda_kb(query: str) -> Dict[str, Any]:\n",
    "    q = query.lower()\n",
    "    hits = []\n",
    "    for k, v in BASE_KB.items():\n",
    "        if k in q or any(tok in v.lower() for tok in q.split()):\n",
    "            hits.append({\"key\": k, \"texto\": v})\n",
    "    return {\"ok\": True, \"hits\": hits[:3]}\n",
    "\n",
    "TOOLS = {\n",
    "    \"calculadora\": tool_calculadora,\n",
    "    \"busqueda_kb\": tool_busqueda_kb,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fee95",
   "metadata": {},
   "source": [
    "#### **4.3 Memoria mínima**\n",
    "Distinguimos dos tipos útiles:\n",
    "\n",
    "- **memoria de trabajo**: contexto actual de la tarea\n",
    "- **memoria episódica**: resultados previos (por ejemplo, cálculos, respuestas confirmadas)\n",
    "\n",
    "Aquí usamos una implementación simple para experimentar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768131a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memoria simple\n",
    "@dataclass\n",
    "class MemoriaAgente:\n",
    "    trabajo: Dict[str, Any] = field(default_factory=dict)\n",
    "    episodica: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "    def guardar_hecho(self, clave: str, valor: Any):\n",
    "        self.trabajo[clave] = valor\n",
    "\n",
    "    def registrar_evento(self, evento: Dict[str, Any]):\n",
    "        self.episodica.append(evento)\n",
    "\n",
    "    def buscar_eventos(self, keyword: str) -> List[Dict[str, Any]]:\n",
    "        k = keyword.lower()\n",
    "        return [e for e in self.episodica if k in str(e).lower()]\n",
    "\n",
    "mem = MemoriaAgente()\n",
    "mem.guardar_hecho(\"tema\", \"alineamiento\")\n",
    "mem.registrar_evento({\"accion\": \"consulta\", \"contenido\": \"Qué es DPO\"})\n",
    "mem.registrar_evento({\"accion\": \"tool\", \"nombre\": \"busqueda_kb\", \"query\": \"dpo\"})\n",
    "mem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366da3d1",
   "metadata": {},
   "source": [
    "#### **4.4 Planificador simple (heurístico)**\n",
    "En investigación, el planificador puede ser:\n",
    "- heurístico (reglas),\n",
    "- prompt-based (ReAct/Plan-and-Execute),\n",
    "- aprendido (policy planner).\n",
    "\n",
    "Aquí usamos un planificador heurístico para aislar variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ac347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planificador heurístico basado en palabras clave\n",
    "def planificar_tarea(pregunta: str) -> List[Dict[str, Any]]:\n",
    "    q = pregunta.lower()\n",
    "    plan = []\n",
    "\n",
    "    if any(sym in q for sym in [\"+\", \"-\", \"*\", \"/\"]):\n",
    "        plan.append({\"accion\": \"usar_tool\", \"tool\": \"calculadora\", \"arg\": pregunta})\n",
    "\n",
    "    if any(k in q for k in [\"rlhf\", \"dpo\", \"orpo\", \"alineamiento\"]):\n",
    "        plan.append({\"accion\": \"usar_tool\", \"tool\": \"busqueda_kb\", \"arg\": pregunta})\n",
    "\n",
    "    if not plan:\n",
    "        plan.append({\"accion\": \"respuesta_directa\", \"arg\": pregunta})\n",
    "\n",
    "    return plan\n",
    "\n",
    "preguntas_demo = [\n",
    "    \"¿Qué es DPO y ORPO?\",\n",
    "    \"Calcula 12 * (5 + 3)\",\n",
    "    \"Explica instruction tuning\"\n",
    "]\n",
    "\n",
    "for p in preguntas_demo:\n",
    "    print(p)\n",
    "    print(planificar_tarea(p))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d4a1a",
   "metadata": {},
   "source": [
    "#### **4.5 Mini-agente ejecutor**\n",
    "\n",
    "Este agente:\n",
    "\n",
    "1. genera un plan,\n",
    "2. ejecuta herramientas,\n",
    "3. guarda memoria,\n",
    "4. produce respuesta final.\n",
    "\n",
    "No usa un LLM real, solo sirve para estudiar **estructura, logs y evaluación**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_toy(pregunta: str, memoria: Optional[MemoriaAgente] = None) -> Dict[str, Any]:\n",
    "    memoria = memoria or MemoriaAgente()\n",
    "    traza = TrazaAgente()\n",
    "\n",
    "    traza.log(f\"Pregunta recibida: {pregunta}\")\n",
    "    plan = planificar_tarea(pregunta)\n",
    "    traza.log(f\"Plan generado: {plan}\")\n",
    "\n",
    "    observaciones = []\n",
    "\n",
    "    for paso in plan:\n",
    "        if paso[\"accion\"] == \"usar_tool\":\n",
    "            nombre = paso[\"tool\"]\n",
    "            arg = paso[\"arg\"]\n",
    "            traza.log(f\"Ejecutando herramienta: {nombre}\")\n",
    "            salida = TOOLS[nombre](arg)\n",
    "            traza.registrar_tool(nombre, {\"arg\": arg}, salida)\n",
    "            memoria.registrar_evento({\"tool\": nombre, \"arg\": arg, \"salida\": salida})\n",
    "            observaciones.append((nombre, salida))\n",
    "\n",
    "        elif paso[\"accion\"] == \"respuesta_directa\":\n",
    "            traza.log(\"No se requiere herramienta; respuesta directa.\")\n",
    "            observaciones.append((\"directo\", {\"texto\": \"Respuesta conceptual: revisa base teórica del cuaderno.\"}))\n",
    "\n",
    "    # Síntesis final \n",
    "    if observaciones and observaciones[0][0] == \"calculadora\":\n",
    "        out = observaciones[0][1]\n",
    "        if out[\"ok\"]:\n",
    "            respuesta = f\"Resultado: {out['resultado']}\"\n",
    "        else:\n",
    "            respuesta = f\"No pude calcular: {out['error']}\"\n",
    "    elif observaciones and observaciones[0][0] == \"busqueda_kb\":\n",
    "        hits = observaciones[0][1][\"hits\"]\n",
    "        if hits:\n",
    "            respuesta = \"Resumen encontrado: \" + \" | \".join(h[\"texto\"] for h in hits)\n",
    "        else:\n",
    "            respuesta = \"No encontré información en la KB local.\"\n",
    "    else:\n",
    "        respuesta = \"Respuesta directa.\"\n",
    "\n",
    "    memoria.registrar_evento({\"tipo\": \"respuesta_final\", \"pregunta\": pregunta, \"respuesta\": respuesta})\n",
    "    return {\"respuesta\": respuesta, \"traza\": traza, \"memoria\": memoria}\n",
    "\n",
    "# Demo\n",
    "mem_demo = MemoriaAgente()\n",
    "res = agente_toy(\"¿Qué es DPO en alineamiento?\", mem_demo)\n",
    "print(\"Respuesta:\", res[\"respuesta\"])\n",
    "print(\"\\nPasos:\")\n",
    "for p in res[\"traza\"].pasos:\n",
    "    print(\"-\", p)\n",
    "print(\"\\nTool calls:\")\n",
    "for tc in res[\"traza\"].tool_calls:\n",
    "    print(tc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bef948",
   "metadata": {},
   "source": [
    "#### **4.6 Agentes con LLM real (plantilla)**\n",
    "Para pasar del agente a uno real, puedes usar:\n",
    "\n",
    "- `transformers` + parsing manual\n",
    "- `LangChain`\n",
    "- `LlamaIndex`\n",
    "- frameworks de agentes (según tu stack)\n",
    "\n",
    "#### **Diseño experimental recomendado**\n",
    "\n",
    "Compara al menos:\n",
    "- **LLM solo**\n",
    "- **LLM + herramientas**\n",
    "- **LLM + herramientas + memoria**\n",
    "- **LLM + herramientas + memoria + planificación explícita**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de protocolo para evaluar agentes (pseudo-código estructurado)\n",
    "protocolo_agentes = {\n",
    "    \"baselines\": [\n",
    "        \"llm_solo\",\n",
    "        \"llm_tools\",\n",
    "        \"llm_tools_memoria\",\n",
    "        \"llm_tools_memoria_plan\"\n",
    "    ],\n",
    "    \"metricas\": [\n",
    "        \"task_success_rate\",\n",
    "        \"tool_precision\",\n",
    "        \"tool_recall\",\n",
    "        \"pasos_promedio\",\n",
    "        \"latencia_total\",\n",
    "        \"errores_de_formato\",\n",
    "        \"hallucination_rate\"\n",
    "    ],\n",
    "    \"datasets_tarea\": [\n",
    "        \"QA con cálculo\",\n",
    "        \"QA con base de conocimiento\",\n",
    "        \"extracción estructurada\",\n",
    "        \"multi-step planning\"\n",
    "    ]\n",
    "}\n",
    "protocolo_agentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e49ca",
   "metadata": {},
   "source": [
    "### **5. Metodología de evaluación para investigación**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ae487",
   "metadata": {},
   "source": [
    "#### **5.1 Matriz de experimentos**\n",
    "\n",
    "Una forma práctica de organizar experimentos:\n",
    "\n",
    "- **Eje A (modelo):** base/SFT/DPO/ORPO  \n",
    "- **Eje B (prompt):** corto/estructurado/con restricciones  \n",
    "- **Eje C (contexto):** 512/1024/2048 tokens  \n",
    "- **Eje D (agente):** sin tools / con tools / con memoria  \n",
    "\n",
    "No cambies 5 cosas a la vez.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de matriz experimental\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "eje_modelo = [\"base\", \"sft\", \"dpo\", \"orpo\"]\n",
    "eje_prompt = [\"simple\", \"estructurado\"]\n",
    "eje_ctx = [512, 1024]\n",
    "eje_agente = [\"sin_tools\", \"con_tools\"]\n",
    "\n",
    "experimentos = []\n",
    "for m, p, c, a in itertools.product(eje_modelo, eje_prompt, eje_ctx, eje_agente):\n",
    "    exp_id = f\"{m}__{p}__ctx{c}__{a}\"\n",
    "    experimentos.append({\n",
    "        \"exp_id\": exp_id,\n",
    "        \"modelo\": m,\n",
    "        \"prompt\": p,\n",
    "        \"ctx\": c,\n",
    "        \"agente\": a,\n",
    "        \"estado\": \"pendiente\"\n",
    "    })\n",
    "\n",
    "df_exp = pd.DataFrame(experimentos)\n",
    "df_exp.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d1dc5",
   "metadata": {},
   "source": [
    "#### **5.2 Intervalos de confianza (bootstrap)**\n",
    "No reportes solo un promedio. Reporta incertidumbre (aunque sea simple).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap para una métrica (ejemplo: accuracy/win-rate)\n",
    "def bootstrap_ci(valores: List[float], n_boot: int = 2000, alpha: float = 0.05, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    arr = np.array(valores, dtype=float)\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.choice(arr, size=len(arr), replace=True)\n",
    "        boots.append(sample.mean())\n",
    "    lo = np.quantile(boots, alpha/2)\n",
    "    hi = np.quantile(boots, 1-alpha/2)\n",
    "    return float(arr.mean()), float(lo), float(hi)\n",
    "\n",
    "valores_demo = [1,1,0,1,1,0,1,1,1,0,1,1]  # éxito/fallo por tarea\n",
    "mean_, lo_, hi_ = bootstrap_ci(valores_demo)\n",
    "print(f\"Win-rate medio={mean_:.3f} | IC95%=({lo_:.3f}, {hi_:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451d6d4",
   "metadata": {},
   "source": [
    "#### **5.3 Registro de errores y análisis cualitativo**\n",
    "\n",
    "Toda investigación con LLMs debe incluir **análisis de errores**.  \n",
    "Sugiere una tabla con:\n",
    "\n",
    "- `id_ejemplo`\n",
    "- `prompt`\n",
    "- `prediccion`\n",
    "- `gold`\n",
    "- `tipo_error`\n",
    "- `causa_probable`\n",
    "- `propuesta_fix`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de tabla de errores\n",
    "errores = pd.DataFrame([\n",
    "    {\n",
    "        \"id_ejemplo\": 1,\n",
    "        \"prompt\": \"Devuelve JSON con 3 frutas\",\n",
    "        \"prediccion\": \"manzana, pera, uva\",\n",
    "        \"gold\": '[\"manzana\",\"pera\",\"uva\"]',\n",
    "        \"tipo_error\": \"formato\",\n",
    "        \"causa_probable\": \"prompt poco restrictivo\",\n",
    "        \"propuesta_fix\": \"forzar esquema JSON y ejemplos few-shot\"\n",
    "    },\n",
    "    {\n",
    "        \"id_ejemplo\": 2,\n",
    "        \"prompt\": \"Explica DPO en 2 líneas\",\n",
    "        \"prediccion\": \"DPO es RLHF con PPO\",\n",
    "        \"gold\": \"DPO optimiza preferencias sin PPO clásico\",\n",
    "        \"tipo_error\": \"conceptual\",\n",
    "        \"causa_probable\": \"confusión terminológica\",\n",
    "        \"propuesta_fix\": \"agregar datos de preferencia curados + evaluación factual\"\n",
    "    }\n",
    "])\n",
    "\n",
    "errores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4684177",
   "metadata": {},
   "source": [
    "### **6. Líneas de investigación y proyectos sugeridos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb8525",
   "metadata": {},
   "source": [
    "#### **6.1 Preguntas de investigación concretas**\n",
    "##### **A. Tokenización y contexto**\n",
    "1. ¿Qué proporción del dataset se trunca a 512/1024/2048 tokens?\n",
    "2. ¿El truncamiento afecta más a ciertas clases o tareas?\n",
    "3. ¿Chunking con solapamiento mejora recuperación de información?\n",
    "\n",
    "##### **B. Instruction tuning**\n",
    "4. ¿Cuál template de instrucción produce mejor cumplimiento de formato?\n",
    "5. ¿Qué mezcla de datasets (general + dominio) mejora más?\n",
    "6. ¿LoRA vs QLoRA cambia calidad o solo costo?\n",
    "\n",
    "##### **C. Alineamiento**\n",
    "7. ¿DPO mejora win-rate sin degradar exactitud factual?\n",
    "8. ¿ORPO reduce errores de formato mejor que SFT puro?\n",
    "9. ¿El modelo se vuelve demasiado \"complaciente\" tras alinear?\n",
    "\n",
    "##### **D. Agentes**\n",
    "10. ¿Las herramientas mejoran éxito real o solo aumentan pasos?\n",
    "11. ¿Qué tipo de memoria ayuda más (episódica vs resumen)?\n",
    "12. ¿La planificación explícita reduce llamadas innecesarias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f3cc4",
   "metadata": {},
   "source": [
    "### **7. Ejercicios orientados a investigación**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1396418",
   "metadata": {},
   "source": [
    "#### **Ejercicios (sin código obligatorio)**\n",
    "\n",
    "1. Define una hipótesis sobre **ventana de contexto** (por ejemplo, 512 vs 1024) y diseña un experimento con variables controladas.  \n",
    "2. Diseña una **rúbrica humana** (4 criterios, escala 1-5) para evaluar respuestas de instruction tuning.  \n",
    "3. Propón un esquema de dataset de **preferencias** para tu dominio (educación, salud, soporte, etc.).  \n",
    "4. Explica qué casos podrían sesgar un modelo de preferencia si los anotadores no tienen guía clara.  \n",
    "5. Diseña una comparación justa entre **SFT**, **DPO** y **ORPO** (mismas seeds, mismos prompts, mismo set de evaluación).  \n",
    "6. Propón una arquitectura de **agente** para una tarea real (por ejemplo,análisis académico, tutor inteligente, soporte técnico).  \n",
    "7. Define métricas para evaluar \"buen uso de herramientas\" (no solo exactitud final).  \n",
    "8. Diseña una tabla de **análisis de errores** con al menos 5 tipos de fallo de LLMs.  \n",
    "9. Propón un plan de mitigación para respuestas factualmente incorrectas tras el alineamiento.  \n",
    "10. Redacta una sección de \"amenazas a la validez\" para tu experimento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b435f",
   "metadata": {},
   "source": [
    "#### **Ejercicios de codificación**\n",
    "\n",
    "1. Implementa una función que calcule **% de truncamiento** por ejemplo y por dataset, y grafique su distribución.  \n",
    "2. Implementa un **formatter** de prompts (3 templates distintos) y compara longitud promedio en tokens.  \n",
    "3. Implementa una evaluación automática de **JSON compliance** con esquema (campos requeridos y tipos).  \n",
    "4. Implementa una versión de **DPO loss** que reciba logits reales de un modelo pequeño.  \n",
    "5. Extiende el `agente_toy` para que use **memoria episódica** al responder preguntas repetidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6b955-fbeb-4706-a7f4-1158b30a5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
